{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e477b3d",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "MODELO ELEGIDO --- LogisticRegression\n",
    "\n",
    "Matriz de confusión:\n",
    "[[106392   2296]\n",
    " [  7854   5808]]\n",
    "\n",
    "Reporte de clasificación:\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0       0.93      0.98      0.95    108688\n",
    "           1       0.72      0.43      0.53     13662\n",
    "\n",
    "    accuracy                           0.92    122350\n",
    "   macro avg       0.82      0.70      0.74    122350\n",
    "weighted avg       0.91      0.92      0.91    122350\n",
    "\n",
    "Precisión del modelo:0.9170412750306498"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c00981fb",
   "metadata": {},
   "source": [
    "Como he explicado anteriormente he decidido elegir el modelo de LogisticRegression. Ahora explicaré lo que significan sus métricas.\n",
    "En cuanto a la matriz de confusión podemos ver que había 106392 reseñas que eran positivas reales y su predicción es acertada, sin embargo vemos que habían 2296 reseñas positivas que él detectó como negativas, el modelo tiene una exhaustividad bastante alta y eso se puede ver reflejado en el recall de 0.98. Sin embargo, con las reseñas negativas la situación cambia, vemos que habían 7854 qu eran negativas y que el modelo detectó como positivas mientras que de 5808 que eran negativas reales las ha predecido como positivas, en este caso la exhaustividad del modelo es más baja como se puede ver en el recall de 0.43.\n",
    "\n",
    "Si nos fijamos en la precisión también muestra resultados acordes con la matrix de confusión, un 0.93 con las reseñas postivas ya que el modelo predice bastante bien mientras que con las negativas la precisión baja a 0.72 lo que también muestra un margen de error más alto en la matriz de confusión.\n",
    "\n",
    "Por otro lado, la clase negativa (0) tiene un f1-score de 0.95 lo que señala que hay un buen balance entre la precisión y el recall. No ocurre lo mismo con la clase positiva (1) ya que el f1-score es de 0.53\n",
    "\n",
    "Como vemos el 0.91% de las predicciones son correctas, lo que es un rendimiento bastante bueno. "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
